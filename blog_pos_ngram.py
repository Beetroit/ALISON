# -*- coding: utf-8 -*-
"""BLOG_POS_ngram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FUbZVDqjOhQ0kvdz_dV0cVrM0Dov_Hx2
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import pandas as pd
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('stopwords')

from nltk.corpus import stopwords
import gc

from nltk import pos_tag
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

from random import random
!pip install transformers -q

import sklearn

from sklearn.metrics import accuracy_score

#Torch stuff
import torch
import torch.distributed as dist
from torch import Tensor
from torch.multiprocessing import Process

from sklearn import metrics
from sklearn.metrics.pairwise import cosine_similarity

import os
import random
from torch.utils.data import Dataset
import time

import pickle
import itertools
from pandas import DataFrame

import torch

# Parallelize apply on Pandas
!pip install pandarallel -q
from pandarallel import pandarallel
pandarallel.initialize()

!pip install captum -q
import captum
from captum.attr import IntegratedGradients

from torchvision import transforms

from transformers import BertTokenizer, BertForMaskedLM

!pip install evaluate -q
import evaluate
!pip install --upgrade --force-reinstall jsonschema==3.0.2

!pip install sentencepiece -q

from sklearn.model_selection import train_test_split

from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

from absl import logging

import tensorflow as tf

import tensorflow_hub as hub
import matplotlib.pyplot as plt
import re
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity as cs

from transformers import AutoModelForSequenceClassification, AutoTokenizer

!pip install bert_score -q
from bert_score import score

import copy

from sklearn import model_selection

from collections import Counter
from nltk import ngrams
import torch.nn.functional as F
import heapq
from heapq import heapify, heappop, heappush
import math
import torch.nn as nn

from matplotlib import rcParams
rcParams['figure.dpi'] = 400

print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))

pip install importlib-metadata

meteor = evaluate.load('meteor')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

gc.collect()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

single_entropy = lambda p, n : p * np.math.log(p, 2) / np.log(n) if p != 0 else 0

def entropy(p, n=10):

    p = [elem / sum(p) for elem in p]
    entropies = []
    for idx in range(len(p)):
        entropies.append(single_entropy(p[idx], n))

    total = -sum(entropies)
    #print('Entropy: ', total)
    return total, [entropy / total for entropy in entropies]

class Model(nn.Module):

    def __init__(self, in_size, num_classes):
        super(Model, self).__init__()

        self.act = nn.ReLU()
        self.dp = 0.40
        self.width = 256
        self.num_classes = num_classes
        self.linear_relu_stack = nn.Sequential(
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(in_size, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, 256),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(256, self.num_classes)
        )

    def forward(self, x):
        return self.linear_relu_stack(x)

replacer_tokenizer = BertTokenizer.from_pretrained("bert-large-uncased")
replacer = BertForMaskedLM.from_pretrained("bert-large-uncased").to(device)

stopwords = set(nltk.corpus.stopwords.words('english'))
case_word = lambda word : word.capitalize() if word in stopwords else word
scramble = lambda word : scrambler(word) if random.random() < 0.5 else word
scrambler = lambda word : word.upper() #if random.random() < 0.5 else word.lower()

'''
axes = [plt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2),
        plt.subplot2grid((2,6), (0,2), colspan=2),
        plt.subplot2grid((2,6), (0,4), colspan=2),
        plt.subplot2grid((2,6), (1,1), colspan=2),
        plt.subplot2grid((2,6), (1,3), colspan=2)]

ax_idx = 4
'''

def tokenize(text):
    ret = []
    for sent in sent_tokenize(text):
        ret.extend(word_tokenize(sent))
    return ret

def removePunct(text):
    text = list(text)
    for punct in ["\?", "\.", "-"]:#, ","]:#, "!", ","]:
        occs = [occ.start() for occ in re.finditer(punct, ''.join(text))]
        occs = random.sample(occs, int(len(occs) * 0.6))
        for occ in occs:
            text[occ] = ''
    return ''.join(text)#re.sub(r'[.?-]', '', text)

def scrambleCase(text):
    return "".join([scramble(char) for char in text if random.random() < 0.2])

def clean(texts):
    ret = []

    for elem in texts:
        elem = removePunct(elem)
        elem = " ".join([sent for sent in sent_tokenize(elem)])
        elem = " ".join([scramble(word) for word in tokenize(elem)])

        #elem = elem.upper()
        elem = elem.replace("  ", "").strip().strip('"').strip("'")

        for punct in ["!", ".", "?", ':', ";", ","]:
            elem = elem.replace(f" {punct}", punct)
        elem = elem.replace("' ", "'")
        elem = elem.replace("#", '')

        elem = elem.replace("( ", "(")
        elem = elem.replace(" )", ")")
        elem = elem.strip()

        ret.append(elem)

    return ret

def replace_interval(words, interval):

    words[interval[0] : interval[1]] =  ["[MASK]"] * (interval[1] - interval[0])
    save = []

    if interval[1] > 512:
        split = interval[1] - 512
        save = words[ :split]
        words = words[split: ]

    inputs = replacer_tokenizer(" ".join(words), return_tensors="pt", padding="max_length", truncation=True, max_length=512).to(device)
    token_ids = replacer_tokenizer.encode(" ".join(words), return_tensors="pt", padding="max_length", truncation=True, max_length=512)

    masked_positions = [idx for idx in range(len(words)) if words[idx] == "[MASK]"]

    outputs = replacer(**inputs)

    predictions = outputs[0]
    sorted_preds, sorted_idx = predictions[0].sort(dim=-1, descending=True)

    predicted_index = [torch.argmax(predictions[0, i]).item() for i in range(len(predictions[0]))]
    predicted_token = [replacer_tokenizer.convert_ids_to_tokens([predicted_index[x]])[0] for x in range(len(predictions[0]))]
    predicted_tokens = [predicted_token[pos] for pos in masked_positions]

    del inputs, token_ids, outputs, predictions, sorted_idx, sorted_preds
    torch.cuda.empty_cache()

    rep_idx = 0
    for word_idx in range(len(words)):
        if words[word_idx] == "[MASK]":
            words[word_idx] = removePunct(predicted_tokens[rep_idx])

    save.extend(words)
    return save

def genPreds(X, adv_tokenizer, adv_classifier):

    preds = []

    for idx in range(len(X)):
        elem = X[idx]
        elem = adv_tokenizer(elem, return_tensors="pt", padding="max_length", truncation=True, max_length=512).to(device)
        with torch.no_grad():
            logits = adv_classifier(**elem).logits
        preds.append(logits.argmax().item())

        del elem
        del logits

        torch.cuda.empty_cache()

    torch.cuda.empty_cache()
    return preds

def dispMetrics(y, predictions, obfuscated_texts, original, incorrect, calculate_entropy=True, title=''):

    y = list(y)
    y_copy = [y[idx] for idx in range(100) if idx not in incorrect]
    predictions_copy = [predictions[idx] for idx in range(100) if idx not in incorrect]
    obfuscated_texts_copy = [obfuscated_texts[idx] for idx in range(100) if idx not in incorrect]
    original_copy = [original[idx] for idx in range(100) if idx not in incorrect]

    score_bert = accuracy_score(y_copy, predictions_copy)
    f1_bert = sklearn.metrics.f1_score(y_copy, predictions_copy, average="macro")

    print("Accuracy: ", score_bert)
    print("F1-score: ", f1_bert)

    #meteor_scores = [0] * len(original_copy)
    use_scores = [0] * len(original_copy)
    #change_rates = [0] * len(obfuscated_texts)

    for idx in range(len(original_copy)):
        #meteor_scores[idx] = meteor.compute(references = [original_copy[idx]], predictions = [obfuscated_texts_copy[idx]])['meteor']
        use_embeddings = embed([original_copy[idx], obfuscated_texts_copy[idx]])
        use_scores[idx] = cs([use_embeddings[0]], [use_embeddings[1]])
        #change_rates[idx] = changeRate(word_tokenize(original[idx]), word_tokenize(obfuscated_texts[idx]))

    #print("Change rate: ", np.mean(change_rates))
    #print("METEOR: ", np.mean(meteor_scores))
    print("USE: ", np.mean(use_scores))
    P, R, F1 = score(obfuscated_texts_copy, original_copy, lang='en', verbose=False)
    print("BERT Score: ", F1.median().item())

    #print(f" & {round(score_bert, 4)} & {round(f1_bert, 4)} & {round(np.mean(meteor_scores), 4)} & {round(float(np.mean(use_scores)), 4)} & {round(F1.median().item(), 4)} \\\\")

    torch.cuda.empty_cache()

    n = max(y) + 1

    confusion = sklearn.metrics.confusion_matrix(y_copy, predictions_copy).tolist()
    cols = [[confusion[row][col] for row in range(n)] for col in range(n)]

    entropies = [entropy(row, n)[0] for row in confusion]

    print(entropies)
    total = sum(entropies)
    entropies = [entropy / total for entropy in entropies]

    print(entropies)

    ticks = [num for num in range(n)]
    plt.bar(ticks, entropies, color ='blue', width = 0.4)
    plt.xticks(ticks)
    plt.xlabel("Authors")
    plt.ylabel("Confusion Probability Entropy")

    print("TOTAL ENTROPY: ", total)

    plt.title(title)

    plt.show()

module_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
USE = hub.load(module_url)
def embed(input):
  return USE(input)

tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']
to_compressed = lambda tag: tags[tag] if tag in tags else tag
idx = 0
temp = dict()
offset = 65
for tag in tags:
    if offset + idx == 90:
        offset = 72
    temp[tag] = chr(offset + idx)
    idx += 1
tags = temp

rev = dict()

for tag in tags.keys():
    rev[tags[tag]] = tag

tokenizers = ["bert-base-cased", "distilbert-base-cased", "roberta-base"]
clfs = ["BERT", "DistilBERT", "RoBERTa"]
adv_paths = {'Avengers' : 'Ensemble/', 'Mutant-X' : 'RFC/'}

class Obfuscator:

    def read_dataset(self):
        head = f'/content/gdrive/MyDrive/PSU_REU/Data/{self.dataset}/'
        path = os.path.join(head, "obf_df.csv")

        if os.path.exists(path):
            new = os.path.join(head, "obf_df_100.csv")
            if not os.path.exists(path):
                obf_df = pd.read_csv(path).drop(['Unnamed: 0'], axis = 1)
                _, obf_df = model_selection.train_test_split(obf_df, test_size=100/len(obf_df), stratify=obf_df['label'], random_state = 1)
                obf_df.to_csv(new)
            else:
                obf_df = pd.read_csv(new).drop(['Unnamed: 0'], axis = 1)

            if self.dataset == 'Blog':
                pass
            elif self.dataset == 'TuringBench':
                obf_df = obf_df.rename(columns={"Generation": "text"})

            self.obf_df = obf_df

        else:
            raise Exception("Dataset does not exist")

    def  __init__(self, dataset):
        self.dataset = dataset
        self.read_dataset()
        self.adv_texts = dict()

        self.features = np.load(f'/content/gdrive/MyDrive/PSU_REU/Data/{dataset}/features.npy')
        if dataset == 'Blog':
            self.num_authors = 10
            self.split = 858
        elif dataset == 'TuringBench':
            self.num_authors = 20
            self.split = 961

    def add_ALISON(self, NUM_NGRAMS_TO_REPLACE=10, MIN_LENGTH=0, constant=1.35):

        obfuscated_texts = []

        isValid = lambda index : index >= self.split#index >= num_char and index < num_char + num_pos
        to_char = lambda x: tags[x] if x in tags else x
        token_and_tag = lambda text: [tup[1] for tup in pos_tag(tokenize(text))]
        token_tag_join = lambda text: ''.join([to_char(tag) for tag in token_and_tag(text)])

        for index in range(len(self.obf_df)):

            [label, text, pos_tags, ranked_indexes] = self.obf_df.iloc[index]

            ranked_indexes = ranked_indexes.strip('][').split(', ')

            ranked_indexes = [float(elem) for elem in ranked_indexes]

            #ranked_indexes = np.add(ranked_indexes, [np.min(ranked_indexes)] * len(ranked_indexes))
            mult = [constant ** len(feature) for feature in self.features]
            ranked_indexes = np.multiply(ranked_indexes, mult)

            ranked_indexes = np.argsort(np.array(ranked_indexes))
            ranked_indexes = [elem for elem in ranked_indexes if isValid(elem)]
            ranked_indexes.reverse()
            to_replace = [self.features[elem] for elem in ranked_indexes]

            to_replace = [replace for replace in to_replace if len(replace) > MIN_LENGTH]
            to_replace = to_replace[ : NUM_NGRAMS_TO_REPLACE]
            '''
            ranked_indexes = [int(elem) for elem in ranked_indexes]
            ranked_indexes = [elem for elem in ranked_indexes if elem >= split]
            to_replace = [features[elem] for elem in ranked_indexes]

            to_replace = [replace for replace in to_replace if len(replace) > 2]
            to_replace = [to_replace[idx] for idx in range(NUM_NGRAMS_TO_REPLACE)]

            to_replace.reverse()
            '''

            sentences = sent_tokenize(text)

            obfuscated_text = ""

            for sentence in sentences:

                words = word_tokenize(sentence)

                retagged = pos_tag(words)
                retagged = [to_compressed(tup[1]) for tup in retagged]

                intervals = []

                for replace in to_replace:

                    '''
                    indices = getIndSkip(replace, retagged)

                    if not (indices == [[]] or indices == None):
                        intervals.extend(indices)

                    '''
                    starts = [i for i in range(len(retagged) - len(replace)) if replace == "".join(retagged[i:i + len(replace)])]

                    for start in starts:
                        intervals.append([start, start + len(replace)])

                changed = [False] * len(words)

                for interval in intervals:

                    if not any(changed[interval[0] : interval[1]]):
                        words = replace_interval(words, interval)
                        changed[interval[0] : interval[1]] = [True] * (interval[1] - interval[0])

                obfuscated_text += " ".join(words)

            torch.cuda.empty_cache()
            obfuscated_texts.append(obfuscated_text)
            #print(obfuscated_text)

        self.adv_texts[f'ALISON:L={NUM_NGRAMS_TO_REPLACE},ML={MIN_LENGTH},c={constant}'] = obfuscated_texts

    def aggregate(self):

        for adv in ['Mutant-X', 'Avengers']:

            head = f'/content/gdrive/MyDrive/PSU_REU/Avengers-Ensemble/obfuscator/{self.dataset}/{adv_paths[adv]}'
            path = f'{head}mlQualitativeResults{self.dataset}{self.num_authors}/'
            texts = []
            os.chdir(path)
            for idx in range(100):
                os.chdir(path)
                if os.path.isdir(f'./{str(idx)}.txt'):
                    os.chdir(f'./{str(idx)}.txt/')
                    os.chdir(f'./{os.listdir(os.getcwd())[0]}/')
                    obf_text = ""
                    if os.path.isfile('./Obfuscated_Text'):
                        with open('./Obfuscated_Text', 'r') as reader:
                            obf_text = reader.readlines()[0]
                    else:
                        for docNum in range(24, -1, -1):
                            if os.path.isfile(f'./BestChangeInIteration_{docNum}'):
                                with open(f'./BestChangeInIteration_{docNum}', 'r') as reader:
                                    obf_text = reader.readlines()[0]
                                break
                    texts.append(obf_text)
                else:
                    texts.append(self.obf_df.iloc[idx, 1])

            self.adv_texts[adv] = texts

        for adv in ['wordCNN', 'wordLSTM']:

            file = f'/content/gdrive/MyDrive/PSU_REU/TextFooler/adv_results/{self.dataset}_{adv}_adversaries.txt'

            texts = []
            with open(file, 'r') as reader:
                lines = reader.readlines()

            for line in lines:
                if 'adv sent (' in line:
                    line = line[line.index(':') + 2 : ]
                    texts.append(line)

            self.adv_texts[adv] = texts

def make_title(method):
    if 'ALISON' in method:
        return 'ALISON'# - Authorwise Confusion Entropy'
    return f'{method}'# - Authorwise Confusion Entropy'

def analyze(obfuscator, adversarial, onlyALISON=False):
    print("============\n", clfs[adversarial])

    adv_tokenizer = AutoTokenizer.from_pretrained(tokenizers[adversarial])

    steps = 12210 if obfuscator.dataset == 'Blog' else 120000
    adv_classifier = AutoModelForSequenceClassification.from_pretrained(f'/content/gdrive/MyDrive/PSU_REU/Models/{obfuscator.dataset}/{clfs[adversarial]}/Blind/checkpoint-{steps}', num_labels=obfuscator.num_authors).to(device)

    predictions = genPreds(list(obfuscator.obf_df['text']), adv_tokenizer, adv_classifier)

    torch.cuda.empty_cache()

    incorrect = [idx for idx in range(100) if predictions[idx] != obfuscator.obf_df.iloc[idx, 0]]

    for method in obfuscator.adv_texts:
        if (onlyALISON and 'ALISON' in method) or not onlyALISON:
            print("------------\n", method)
            texts = obfuscator.adv_texts[method]
            if 'ALISON' in method:
                texts = clean(texts)
            print(texts)
            dispMetrics(obfuscator.obf_df['label'], genPreds(texts, adv_tokenizer, adv_classifier), [text.lower() for text in texts], obfuscator.obf_df['text'], incorrect, title=make_title(method))

    del adv_classifier

import pandas as pd

df = pd.read_csv(f'/content/gdrive/MyDrive/PSU_REU/Data/Blog/obf_df.csv')

for idx, row in df.iterrows():
    print(row[2].strip()[:200], '\n')

test = Obfuscator("TuringBench")

test.add_ALISON(NUM_NGRAMS_TO_REPLACE=12, MIN_LENGTH=0, constant=1.35)

test.add_ALISON(NUM_NGRAMS_TO_REPLACE=16, MIN_LENGTH=0, constant=1.35)

test.add_ALISON(NUM_NGRAMS_TO_REPLACE=2, MIN_LENGTH=0, constant=1.35)

test.add_ALISON(NUM_NGRAMS_TO_REPLACE=8, MIN_LENGTH=0, constant=1.35)

test.add_ALISON(NUM_NGRAMS_TO_REPLACE=0, MIN_LENGTH=0, constant=1.35)

test.aggregate()

analyze(test, 0)

analyze(test, 1)

analyze(test, 2)

blog = Obfuscator("Blog")

blog.add_ALISON(NUM_NGRAMS_TO_REPLACE=2, MIN_LENGTH=0, constant=1.35)

blog.aggregate()

analyze(blog, 0)



analyze(blog, 0)

analyze(blog, 1)

analyze(blog, 2)

isValid = lambda index : index >= split#index >= num_char and index < num_char + num_pos
to_char = lambda x: tags[x] if x in tags else x
token_and_tag = lambda text: [tup[1] for tup in pos_tag(tokenize(text))]
token_tag_join = lambda text: ''.join([to_char(tag) for tag in token_and_tag(text)])

def show_entropy(y_ground, y_pred, title = "Authorwise Confusion Entropy"):

    confusion = sklearn.metrics.confusion_matrix(y_ground, y_pred).tolist()
    cols = [[confusion[row][col] for row in range(10)] for col in range(10)]

    entropies = [entropy(row)[0] for row in confusion]

    plt.bar([num for num in range(10)], entropies, color ='blue', width = 0.4)
    plt.xlabel("Authors")
    plt.ylabel("Confusion Probability Entropy")

    print("TOTAL ENTROPY: ", sum(entropies))

    plt.title(title)
    plt.show()

def run_entropy(constants, adversarial, nums, min_length=1, first=100):

    print('Calculating Entropy')

    adv_tokenizer = AutoTokenizer.from_pretrained(tokenizers[adversarial])
    adv_classifier = AutoModelForSequenceClassification.from_pretrained(f'/content/gdrive/MyDrive/PSU_REU/Models/BLOG/{clfs[adversarial]}/Blind/checkpoint-12210', num_labels=10).to(device)

    predictions = genPreds(list(X_ref['text']), y_ref, adv_tokenizer, adv_classifier)

    incorrect = []
    correct = []
    for idx in range(first):
        if(predictions[idx] != y_ref.iloc[idx]):
            incorrect.append(idx)
        else:
            correct.append(idx)
    for idx in range(first, 100):
        incorrect.append(idx)

    print(f'Producing {len(correct)} samples')
    X = X_ref.drop([X_ref.index[idx] for idx in incorrect])
    y = y_ref.drop([y_ref.index[idx] for idx in incorrect])

    freqs = [list(y).count(idx) for idx in range(10)]
    before_entropies = entropy(freqs, 10)
    before_entropies[0] += 0.005

    for num_ngrams in [11]:
        print('------------\n', 'NUM_NGRAMS: ', num_ngrams)
        obfuscated_texts = run_trial(X, y, num_ngrams, min_length, 1.35)
        predictions = genPreds(obfuscated_texts, y, adv_tokenizer, adv_classifier)

        freqs = [predictions.count(idx) for idx in range(10)]
        after_entropies = entropy(freqs, 10)

    width = 0.4
    fig, ax = plt.subplots()
    plt.bar([num - 0.5 * width for num in range(10)], before_entropies, color ='blue', width = width, label="Pre-Obfuscation", edgecolor="Black")

    ax.set_xlabel("Authors")
    ax.set_ylabel("Normalized Entropy")
    ax.grid(False)

    ax.set_xticks([num for num in range(10)])

    plt.bar([num + width / 2 for num in range(10)], after_entropies, color ='orange', width = width, label = "Post-Obfuscation", edgecolor="Black")

    plt.title('Blog Authorship Corpus Normalized Label Entropy')
    plt.legend(loc='lower left')
    plt.show()

num_so_far = 100

exp([1.35], 0, [5, 6, 7, 8, 9, 10, 11], min_length = 0)

width = 0.4
fig, ax = plt.subplots()
plt.bar([num - 0.5 * width for num in range(20)], before_entropies, color ='blue', width = width, label="Pre-Obfuscation", edgecolor="Black")

ax.set_xlabel("Authors")
ax.set_ylabel("Normalized Entropy")
ax.grid(False)

ax.set_xticks([num for num in range(20)])

plt.bar([num + width / 2 for num in range(20)], after_entropies, color ='orange', width = width, label = "Post-Obfuscation", edgecolor="Black")

plt.title('Blog Authorship Corpus Normalized Label Entropy')
plt.legend()
plt.show()

exp([1.3818181818181818], 1, [16, 17, 18, 19, 20], min_length=2)

exp([1.35], 2, [11], min_length=0)

def percent_increase(ours, other):
    diff = ours - other

    print(' (' + str(round(diff / other * 100, 2)) + '\%)')

percent_increase(0.9452, 0.9382)

def countSkip(skipgram, text):

  m = len(skipgram)
  n = len(text)

  mat = [[0 for i in range(n + 1)] for j in range(m + 1)]

  for j in range(n + 1):
    mat[0][j] = 1

  for i in range(1, m + 1):
    for j in range(1, n + 1):
      mat[i][j] = mat[i][j - 1]

      if skipgram[i - 1] == text[j - 1]:
        mat[i][j] += mat[i - 1][j - 1]

  return mat[m][n]

def getIndSkip(skipgram, text):

    m = len(skipgram)
    n = len(text)

    mat = [[None for i in range(n + 1)] for j in range(m + 1)]
    for i in range(n + 1):
        mat[0][i] = [[]]

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if skipgram[i - 1] == text[j - 1]:
                if mat[i - 1][j - 1] == None:
                    mat[i][j] = None
                else:
                    if mat[i][j - 1] == None:
                        mat[i][j] = copy.deepcopy(mat[i - 1][j - 1])
                        for arr in mat[i][j]:
                            arr.append(j - 1)
                    elif mat[i - 1][j - 1] == None and mat[i][j - 1] != None:
                        mat[i][j] = mat[i][j - 1]
                    else:
                        mat[i][j] = copy.deepcopy(mat[i - 1][j - 1])
                        for arr in mat[i][j]:
                            arr.append(j - 1)
                        mat[i][j].extend(mat[i][j - 1])

            else:
                mat[i][j] = mat[i][j - 1] if mat[i][j - 1] != None else None

    return mat[m][n]

def find_freq_n_gram_in_txt(text, pos_text, n_gram_lengths, n_grams, pos_n_gram_lengths, pos_n_grams, word_n_gram_lengths, word_n_grams):

    num_ngrams = sum([len(element) for element in n_grams]) + sum([len(element) for element in pos_n_grams]) + sum([len(element) for element in word_n_grams])
    to_ret = np.zeros([num_ngrams])
    ret_idx = 0

    for idx in range(len(n_grams)):
        num_ngrams = len(Counter(ngrams(text, n_gram_lengths[idx])))

        for n_gram in n_grams[idx]:
            to_ret[ret_idx] = text.count(''.join(n_gram)) / num_ngrams if num_ngrams != 0 else 0
            ret_idx += 1

    for idx in range(len(pos_n_grams)):
        num_pos_ngrams = len(Counter(ngrams(pos_text, pos_n_gram_lengths[idx])))

        for pos_n_gram in pos_n_grams[idx]:
            to_ret[ret_idx] = pos_text.count(''.join(pos_n_gram)) / num_pos_ngrams if num_pos_ngrams != 0 else 0
            ret_idx += 1

    words = tokenize(text)
    spaced_text = text
    for idx in range(len(word_n_grams)):
        num_word_ngrams = len(Counter(ngrams(words, word_n_gram_lengths[idx])))

        for word_ngram in word_n_grams[idx]:
            to_ret[ret_idx] = spaced_text.count(' '.join(word_ngram)) / num_word_ngrams if num_word_ngrams != 0 else 0
            ret_idx += 1

    return to_ret
