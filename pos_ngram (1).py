# -*- coding: utf-8 -*-
"""POS_ngram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16wKnkfSW9jKLlHF3TE1F--UrK8lJuB7I
"""

!pip3 install transformers

import numpy as np
import pandas as pd
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
import gc

from nltk import pos_tag
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

from random import random
import torch
'''
from transformers import BartForConditionalGeneration, BartTokenizer
'''
import csv
import time

from nltk.util import ngrams
from collections import Counter
import heapq

from string import punctuation
from nltk.corpus import stopwords

from google.colab import files
from torchvision import transforms
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.colab import drive
import gspread

import sklearn

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn import preprocessing
import xgboost as xgb

from tqdm import tqdm_notebook
from tqdm import trange

#Math stuff
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

#Torch stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import transforms
import torchvision.transforms.functional as tf
from torchvision import datasets, transforms, models
from torch.optim.lr_scheduler import ReduceLROnPlateau

import copy
import csv

from torchvision import transforms
from sklearn import metrics
from tqdm import trange
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import normalized_mutual_info_score

import os
import random
from torch.utils.data import Dataset
from tqdm import tqdm
import time
import nvidia_smi

auth.authenticate_user()

drive.mount('/content/gdrive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

gc.collect()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from functools import partial
from tqdm import tqdm
tqdm = partial(tqdm, position=0, leave=True)

import warnings
warnings.filterwarnings("ignore")

tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']
cost = lambda x, t: (2*((x-t)/(x+t)))**2

class NeuralNetwork(nn.Module):
    def __init__(self, in_size, num_classes):
        super(NeuralNetwork, self).__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(in_size, 1024),
            nn.ReLU(),
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.Linear(512, num_classes),
        )

    def forward(self, x):
        return self.linear_relu_stack(x)

class Loader(Dataset):

    def __init__(self, x, y, top_idxs, Scaler):

        self.x = torch.Tensor(Scaler.transform(x))
        self.y = torch.Tensor([top_idxs[val] for val in y])

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

def train_and_eval(model, EPOCHS, training_set, validation_set, loss_function, optimizer, scheduler):

    model.to(device)
    
    for epoch in range(1, EPOCHS + 1):
        print('------------', '\n', 'Epoch #:', epoch, '\n', 'Training:')

        with torch.set_grad_enabled(True):

            total = 0
            correct = 0

            model.train()

            labels = []
            predictions = []
            all_preds = []
            all_labels = []

            with tqdm(training_set, unit = "batch", bar_format = '{l_bar}{bar:20}{r_bar}{bar:-20b}') as tqdm_train:
                for training_data, labels in tqdm_train:
                    tqdm_train.set_description(f'Epoch: {epoch}')

                    training_data = training_data.to(device)
                    labels = torch.tensor(labels, dtype = torch.long)
                    #print(labels)
                    labels = labels.to(device)
                    #print(correct_prediction)

                    optimizer.zero_grad()

                    prediction = model(training_data).squeeze(1)

                    loss = loss_function(prediction, labels)

                    loss.backward()
                    optimizer.step()

                    for idx, i in enumerate(prediction):
                        if torch.argmax(i) == labels[idx]:
                            correct += 1
                        total += 1

                    tqdm_train.set_postfix(loss=loss.item(), accuracy=100.*correct/total)
                    time.sleep(0.1)

            training_accuracy = round(correct/total, 5)
            print('Training Accuracy: ', training_accuracy)

            scheduler.step()


        print('------------', '\n', 'Validation:')
        model.eval()
        gc.collect()

        total = 0
        correct = 0

        with torch.no_grad():

            with tqdm(validation_set, unit="batch", bar_format = '{l_bar}{bar:20}{r_bar}{bar:-20b}') as tqdm_val:
                tqdm_val.set_description(f'Epoch: {epoch}')
                for val_data, labels in tqdm_val:

                    val_data = val_data.to(device)

                    labels = torch.tensor(labels, dtype = torch.long)
                    labels = labels.to(device)

                    prediction = model(val_data).squeeze(1)

                    for idx, i in enumerate(prediction):
                        if torch.argmax(i) == labels[idx]:
                            correct += 1
                        total += 1

                    tqdm_val.set_postfix(accuracy=100.*correct/total)
                    time.sleep(0.1)

            validation_accuracy = round(correct/total, 5)
            print('Validation Accuracy: ', validation_accuracy)
            gc.collect()
    return model

idx = 0
temp = dict()
offset = 65
for tag in tags:
    if offset + idx == 90:
        offset = 72
    temp[tag] = chr(offset + idx)
    idx += 1
tags = temp

def return_best_pos_n_grams(n, L, text):

    words = word_tokenize(text)
    pos_tokens = pos_tag(words)

    text = [tup[1] for tup in pos_tokens]
    text = ' '.join(text)

    n_grams = ngrams(text, n)

    data = dict(Counter(n_grams))

    print('Number n-grams: ', len(data))

    list_ngrams = heapq.nlargest(L, data.keys(), key=lambda k: data[k])
    return list_ngrams


def return_best_n_grams(n, L, text):
    bigrams = ngrams(text, n)

    data = dict(Counter(bigrams))
    list_ngrams = heapq.nlargest(L, data.keys(), key=lambda k: data[k])
    return list_ngrams

def find_freq_n_gram_in_txt(text, pos_text, n_gram_lengths, n_grams, pos_n_gram_lengths, pos_n_grams):
    
    to_ret = []

    for idx in range(len(n_grams)):
        num_ngrams = len(Counter(ngrams(text, n_gram_lengths[idx])))

        for n_gram in n_grams[idx]: 
            to_ret.append(text.count(''.join(n_gram))/num_ngrams)

    for idx in range(len(pos_n_grams)):
        num_pos_ngrams = len(Counter(ngrams(pos_text, pos_n_gram_lengths[idx])))

        for pos_n_gram in pos_n_grams[idx]: 
            to_ret.append(pos_text.count(''.join(pos_n_gram))/num_pos_ngrams)

    return to_ret

def get_top_idx(freqs, k):

    freqs = [(idx, freqs[idx]) for idx in range(len(freqs))]
    freqs.sort(key = lambda val: val[1], reverse = True)
    
    return [freqs[idx][0] for idx in range(k)]

authors = ['allen-p', 'arnold-j', 'arora-h', 'badeer-r', 'bailey-s', 'bass-e', 'baughman-d', 'beck-s', 'benson-r', 'blair-l', 'brawner-s', 'buy-r', 'campbell-l', 'carson-m', 'cash-m', 'causholli-m', 'corman-s', 'crandell-s', 'cuilla-m', 'dasovich-j', 'davis-d', 'dean-c', 'delainey-d', 'derrick-j', 'dickson-s', 'donoho-l', 'donohoe-t', 'dorland-c', 'ermis-f', 'farmer-d', 'fischer-m', 'forney-j', 'fossum-d', 'gang-l', 'gay-r', 'geaccone-t', 'germany-c', 'gilbertsmith-d', 'giron-d', 'griffith-j', 'grigsby-m', 'guzman-m', 'haedicke-m', 'hain-m', 'harris-s', 'hayslett-r', 'heard-m', 'hendrickson-s', 'hernandez-j', 'hodge-j', 'holst-k', 'horton-s', 'hyatt-k', 'hyvl-d', 'jones-t', 'kaminski-v', 'kean-s', 'keavey-p', 'keiser-k', 'king-j', 'kitchen-l', 'kuykendall-t', 'lavorato-j', 'lay-k', 'lenhart-m', 'lewis-a', 'linder-e', 'lokay-m', 'lokey-t', 'love-p', 'lucci-p', 'maggi-m', 'mann-k', 'martin-t', 'may-l', 'mccarty-d', 'mcconnell-m', 'mckay-b', 'mckay-j', 'mclaughlin-e', 'merriss-s', 'meyers-a', 'mims-thurston-p', 'motley-m', 'neal-s', 'nemec-g', 'panus-s', 'parks-j', 'pereira-s', 'perlingiere-d', 'phanis-s', 'pimenov-v', 'platter-p', 'presto-k', 'quenet-j', 'quigley-d', 'rapp-b', 'reitmeyer-j', 'richey-c', 'ring-a', 'ring-r', 'rodrique-r', 'rogers-b', 'ruscitti-k', 'sager-e', 'saibi-e', 'salisbury-h', 'sanchez-m', 'sanders-r', 'scholtes-d', 'schoolcraft-d', 'schwieger-j', 'scott-s', 'semperger-c', 'shackleton-s', 'shankman-j', 'shapiro-r', 'shively-h', 'skilling-j', 'slinger-r', 'smith-m', 'solberg-g', 'south-s', 'staab-t', 'stclair-c', 'steffes-j', 'stepenovitch-j', 'stokley-c', 'storey-g', 'sturm-f', 'swerzbin-m', 'symes-k', 'taylor-m', 'tholt-j', 'thomas-p', 'townsend-j', 'tycholiz-b', 'ward-k', 'watson-k', 'weldon-c', 'whalley-g', 'whalley-l', 'white-s', 'whitt-m', 'williams-j', 'williams-w3', 'wolfe-j', 'ybarbo-p', 'zipper-a', 'zufferli-j']
temp = dict()

for idx in range(len(authors)):
    temp[authors[idx]] = idx

authors = temp

data = pd.read_csv('/content/gdrive/MyDrive/PSU REU/Data/EnronEmails/smaller_nospace.csv')

print(data.head())

def tag_data(data):
    to_char = lambda x: tags[x] if x in tags else x
    token_and_tag = lambda text: [tup[1] for tup in pos_tag(word_tokenize(text))]
    token_tag_join = lambda text: ''.join([to_char(tag) for tag in token_and_tag(text)])

    data['POS Tagged'] = data['Generation'].apply(token_tag_join)

    return data

def preprocess_data(data, num_authors, num_total, tag = True):

    print('------------', '\n', 'Tagging...')
    data = tag_data(data) if tag else data

    print('------------', '\n', 'Counting and aggregating texts...')
    number_texts = [0 for idx in range(num_total)]

    texts = ['' for idx in range(num_total)]
    pos_texts = ['' for idx in range(num_total)]

    stop_words = set(stopwords.words('english'))

    for index, row in data.iterrows():
        number_texts[int(row[0])] += 1
        filtered_sentence = row[1]
        filtered_pos_sentence = row[2]
        '''
        filtered_sentence = ' '.join([w for w in filtered_sentence.split() if not w in stop_words])
        filtered_sentence = ''.join([w for w in filtered_sentence if w not in set(punctuation)])
        '''
        filtered_sentence = filtered_sentence.strip()
        texts[int(row[0])] += ' ' + filtered_sentence
        pos_texts[int(row[0])] += filtered_pos_sentence

    print(number_texts)

    top_idxs = get_top_idx(number_texts, num_authors)

    class_weights = [number_texts[author] for author in top_idxs]

    total  = [texts[idx] for idx in top_idxs]
    total = ' '.join(total)
    pos_total = [pos_texts[idx] for idx in top_idxs]
    pos_total = ''.join(pos_total)

    temp = dict()
    for idx in range(len(top_idxs)):
        temp[top_idxs[idx]] = idx
        
    top_idxs = temp

    print('------------', '\n', 'Preprocessing complete!')

    return top_idxs, total, pos_total

def gen_data(data, lengths, pos_lengths, top_idxs, total, pos_total):

    print('------------', '\n', 'Generating n-grams...')

    n_grams = [return_best_n_grams(n, 150, total) for n in lengths]

    print('------------', '\n', 'Generating POS n-grams...')

    pos_n_grams = [return_best_n_grams(n, 150, pos_total) for n in pos_lengths]

    print('------------', '\n', 'Generating data...')
    X = []
    y = []
    for index, row in data.iterrows():
        if int(row[0]) in top_idxs:
            y.append(top_idxs[int(row[0])])
            X.append(find_freq_n_gram_in_txt(row[1], row[2], lengths, n_grams, pos_lengths, pos_n_grams))

    X = np.array(X)
    y = np.array(y)

    return X, y #sklearn.model_selection.train_test_split(X, y, test_size = 0.15)

from sklearn.metrics import f1_score
from sklearn.

def train_and_test(X_train, X_test, y_train, y_test):
    print('------------', '\n', 'Training...')
    #model = xgb.XGBClassifier().fit(X_train, y_train)
    model = LogisticRegression(random_state=0, max_iter = 10000).fit(X_train, y_train)
    print('------------', '\n', 'Validating')
    y_pred = model.predict(X_test)
    print('------------')
    print('Accuracy: ', accuracy_score(y_test, y_pred))
    #print('F1 Score', f1_score(y_test, y_pred))
    print('------------', '\n', 'Complete!', '\n', '------------')
    return y_test, y_pred

gc.collect()
'''
top_idxs, total, pos_total = preprocess_data(data, 20, 20, tag = False)
gc.collect()

X_train, X_test, y_train, y_test = gen_data(data, [], [1, 2, 3, 4, 5, 6], top_idxs, total, pos_total)
train_and_test(X_train, X_test, y_train, y_test)
'''

gc.collect()

BATCH_SIZE = 512
num_authors = 5

top_idxs, total, pos_total, class_weights = preprocess_data(data, num_authors, 150, tag = False)
print(top_idxs)
X_train, X_test, y_train, y_test = gen_data(data, [1, 2, 3, 4], [1, 2, 3, 4], top_idxs, total, pos_total)

print('------------', '\n', 'Scaling, Loading, and Shuffling Data')
Scaler = sklearn.preprocessing.StandardScaler().fit(X_train)
training_Loader = Loader(X_train, y_train, top_idxs, Scaler)
validation_Loader = Loader(X_test, y_test, top_idxs, Scaler)

training_set = torch.utils.data.DataLoader(training_Loader, batch_size = BATCH_SIZE, shuffle = True)
validation_set = torch.utils.data.DataLoader(validation_Loader, batch_size = BATCH_SIZE, shuffle = False)

print(len(X_train))

class_weights=torch.tensor(class_weights, dtype=torch.float)

#weight = class_weights)

model = NeuralNetwork(len(X_train[0]), num_authors)

optimizer = optim.SGD(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)

scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)

#train_and_eval(model, EPOCHS, training_set, validation_set, loss_function, optimizer, scheduler)

_ = '''
to_write = []
all_tags = set()

to_char = lambda x: tags[x] if x in tags else x

for index, row in data.iterrows():
    words = word_tokenize(row[1])
    words = pos_tag(words)
    words = [tup[1] for tup in words]
    words = [to_char(word) for word in words]
    words = ' '.join(words)
    to_write.append([row[0], row[1], words])

      
with open('/content/gdrive/MyDrive/PSU REU/Data/EnronEmails/smaller.csv', 'w', encoding='UTF8', newline='') as f:
    writer = csv.writer(f)

    writer.writerow(['AuthorID', 'Message', 'POS_Tagged'])

    writer.writerows(to_write)
'''

'''test([1, 2, 3, 4], [1, 2, 3, 4])
#50'''

'''
model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')

def obsfucate(target, n, M, costs, prediction):

    targets = costs[prediction]
    targets = targets[targets[ : , 1 ].argsort()]

    targets = [row[1] for row in targets]
    targets = targets[ :M]

    sentences = sent_tokenize(target)

    text = ''

    for sentence in sentences:
        words = word_tokenize(sentence)
        tags = pos_tag(words)
        tags = ''.join(tags)

        if any([n_gram in tags for n_gram in targets]):
            batch = tokenizer(sentence, return_tensors='pt')
            generated_ids = model.generate(batch['input_ids'])
            text += tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        else:
            text += sentence

    return text
'''

gc.collect()

authors = []
for idx, row in test.iterrows():
    if row[0] not in authors:
        authors.append(row[0])

print(authors)

print(len(authors))

temp = dict()
for idx in range(len(authors)):
    temp[authors[idx]] = idx
authors = temp

get_idx = lambda x: authors[x]

def idk(data):
    data = tag_data(data)
    data['label'] = data['label'].apply(get_idx)

    return data

train = idk(train)
test = idk(test)
val = idk(val)

train = pd.read_csv('/content/gdrive/MyDrive/PSU REU/Data/TuringBench/ProcessedAA/train.csv')
test = pd.read_csv('/content/gdrive/MyDrive/PSU REU/Data/TuringBench/ProcessedAA/test.csv')
val = pd.read_csv('/content/gdrive/MyDrive/PSU REU/Data/TuringBench/ProcessedAA/val.csv')

train = train.drop(['Unnamed: 0'], axis = 1)
test = test.drop(['Unnamed: 0'], axis = 1)
val = val.drop(['Unnamed: 0'], axis = 1)

print(train.head())

rearrange = lambda df: df[['label', 'Generation', 'POS Tagged']]

train = rearrange(train)
test = rearrange(test)
val = rearrange(val)

train = pd.read_csv('/content/gdrive/MyDrive/PSU REU/Data/TuringBench/ProcessedAA/train.csv')
test = pd.read_csv('/content/gdrive/MyDrive/PSU REU/Data/TuringBench/ProcessedAA/test.csv')
val = pd.read_csv('/content/gdrive/MyDrive/PSU REU/Data/TuringBench/ProcessedAA/val.csv')

top_idxs, total, pos_total = preprocess_data(train, 20, 20, tag = False)

print(get_top_idx([7914, 7295, 7295, 7914, 7295, 7148, 7910, 7295, 7914, 7383, 7388, 7914, 7295, 7295, 7295, 7914, 7388, 7914, 7152, 7388], 20))

print(top_idxs)

train = pd.concat([train, val])

X_train, y_train = gen_data(train, [1, 2, 3, 4], [1, 2, 3, 4], top_idxs, total, pos_total)
X_test, y_test = gen_data(test, [1, 2, 3, 4], [1, 2, 3, 4], top_idxs, total, pos_total)

y_test, y_pred = train_and_test(X_train, X_test, y_train, y_test)

print('------------', '\n', 'Scaling, Loading, and Shuffling Data')
Scaler = sklearn.preprocessing.StandardScaler().fit(X_train)
training_Loader = Loader(X_train, y_train, top_idxs, Scaler)
validation_Loader = Loader(X_test, y_test, top_idxs, Scaler)

training_set = torch.utils.data.DataLoader(training_Loader, batch_size = BATCH_SIZE, shuffle = True)
validation_set = torch.utils.data.DataLoader(validation_Loader, batch_size = BATCH_SIZE, shuffle = False)

EPOCHS = 100
STEP_SIZE = 5
GAMMA = 0.1
LR = 0.005
WEIGHT_DECAY = 0.05
MOMENTUM = 0.9

num_authors = 20

model = NeuralNetwork(len(X_train[0]), num_authors)

loss_function = nn.CrossEntropyLoss()

optimizer = optim.SGD(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)

scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)

train_and_eval(model, EPOCHS, training_set, validation_set, loss_function, optimizer, scheduler)

X_train, y_train = gen_data(train, [1, 2, 3, 4], [1, 2, 3, 4], top_idxs, total, pos_total)
X_test, y_test = gen_data(test, [1, 2, 3, 4], [1, 2, 3, 4], top_idxs, total, pos_total)

y_test, y_pred = train_and_test(X_train, X_test, y_train, y_test)

loss_function = nn.CrossEntropyLoss()

print(len(X_test))

print(len(test))

for i in range(20):
    print(i in y_test)

