# -*- coding: utf-8 -*-
"""POS_ngramAA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Pot4BiHdv2K8SCxGd5AI9zKogUgM86O
"""

import numpy as np
import pandas as pd
import nltk
from nltk import skipgrams
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
import gc

from nltk import pos_tag
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tokenize.regexp import regexp_tokenize

from random import random
import torch

import csv
import time

from nltk.util import ngrams
from collections import Counter
import heapq

from string import punctuation
from nltk.corpus import stopwords

from torchvision import transforms
from google.colab import drive

import sklearn

from sklearn.metrics import accuracy_score
from sklearn import preprocessing

#Math stuff
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

#Torch stuff
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import transforms
import torchvision.transforms.functional as tf
from torchvision import datasets, transforms, models
from torch.optim.lr_scheduler import ReduceLROnPlateau

import copy
import csv

from torchvision import transforms
from sklearn import metrics
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import normalized_mutual_info_score

import os
import random
from torch.utils.data import Dataset
import time

import pickle
import itertools
from pandas import DataFrame

import torch

# Parallelize apply on Pandas
!pip install pandarallel
from pandarallel import pandarallel
pandarallel.initialize()

!pip install captum
import captum


from functools import partial
from tqdm import tqdm
tqdm = partial(tqdm, position=0, leave=True)

import warnings
warnings.filterwarnings("ignore")

from itertools import chain
import copy

import pickle

drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/Colab Notebooks

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

gc.collect()

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']

class Model(nn.Module):

    def __init__(self, in_size, num_classes):
        super(Model, self).__init__()

        self.act = nn.ReLU()
        self.dp = 0.40
        self.width = 512
        self.num_classes = num_classes
        self.linear_relu_stack = nn.Sequential(
            nn.Dropout(p = self.dp),
            nn.Linear(in_size, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, self.width),
            self.act,
            nn.Dropout(p = self.dp),
            nn.Linear(self.width, 128),
            self.act,
            nn.Linear(128, self.num_classes)
        )

    def forward(self, x):
        return self.linear_relu_stack(x)

def tokenize(text):
    ret = []
    for sent in sent_tokenize(text):
        ret.extend(word_tokenize(sent))
    return ret

class Loader(Dataset):
    def __init__(self, x, y):
        self.x = torch.nan_to_num(torch.Tensor(x))
        self.y = torch.Tensor(y)#[top_idxs[val] for val in y])

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

def train_and_eval(model, EPOCHS, training_set, validation_set, loss_function, optimizer, scheduler, save_path, save_epoch):

    model.to(device)

    for epoch in range(1, EPOCHS + 1):
        print('------------', '\n', 'Epoch #:', epoch, '\n', 'Training:')

        with torch.set_grad_enabled(True):

            total = 0
            correct = 0

            model.train()

            labels = []
            predictions = []
            all_preds = []
            all_labels = []

            with tqdm(training_set, unit = "batch", bar_format = '{l_bar}{bar:20}{r_bar}{bar:-20b}') as tqdm_train:
                for training_data, labels in tqdm_train:
                    tqdm_train.set_description(f'Epoch: {epoch}')

                    training_data = training_data.to(device)
                    labels = torch.tensor(labels, dtype = torch.long)
                    #print(labels)
                    labels = labels.to(device)
                    #print(correct_prediction)

                    optimizer.zero_grad()

                    prediction = model(training_data).squeeze(1)

                    loss = loss_function(prediction, labels)

                    loss.backward()
                    optimizer.step()

                    for idx, i in enumerate(prediction):
                        if torch.argmax(i) == labels[idx]:
                            correct += 1
                        total += 1

                    tqdm_train.set_postfix(loss=loss.item(), accuracy=100.*correct/total)
                    time.sleep(0.1)

            training_accuracy = round(correct/total, 5)
            print('Training Accuracy: ', training_accuracy)

            scheduler.step()


        print('------------', '\n', 'Validation:')
        model.eval()
        gc.collect()

        total = 0
        correct = 0

        with torch.no_grad():

            with tqdm(validation_set, unit="batch", bar_format = '{l_bar}{bar:20}{r_bar}{bar:-20b}') as tqdm_val:
                tqdm_val.set_description(f'Epoch: {epoch}')
                for val_data, labels in tqdm_val:

                    val_data = val_data.to(device)

                    labels = torch.tensor(labels, dtype = torch.long)
                    labels = labels.to(device)

                    prediction = model(val_data).squeeze(1)

                    for idx, i in enumerate(prediction):
                        if torch.argmax(i) == labels[idx]:
                            correct += 1
                        total += 1

                    tqdm_val.set_postfix(accuracy=100.*correct/total)
                    time.sleep(0.1)

            validation_accuracy = round(correct/total, 5)
            print('Validation Accuracy: ', validation_accuracy)
            gc.collect()

        if epoch % save_epoch == 0:
            if not os.path.isdir(save_path):
                os.path.makedirs(save_path)
            torch.save(model.state_dict(), os.path.join(save_path, f'model_{epoch}.pt'))

    return model

idx = 0
temp = dict()
offset = 65
for tag in tags:
    if offset + idx == 90:
        offset = 72
    temp[tag] = chr(offset + idx)
    idx += 1
tags = temp

def countSkip(skipgram, texts):

    total = 0
    m = len(skipgram)

    for text in texts:

        n = len(text)

        mat = [[0 for i in range(n + 1)] for j in range(m + 1)]
        for j in range(n + 1):
            mat[0][j] = 1

        for i in range(1, m + 1):
            for j in range(1, n + 1):
                mat[i][j] = mat[i][j - 1]

                if skipgram[i - 1] == text[j - 1]:
                    mat[i][j] += mat[i - 1][j - 1]

        total += mat[m][n]

    return total

def get_skipgrams(text, n, k):
    '''
    if n > 1:
        ans = [skipgram for skipgram in skipgrams(text, n, k)]
    else:
        ans = ngrams(text, n)
    return ans
    '''
    return ngrams(text, n)

def return_best_pos_n_grams(n, L, pos_texts):
    n_grams = ngrams(pos_texts, n)

    data = dict(Counter(n_grams))
    list_ngrams = heapq.nlargest(L, data.keys(), key=lambda k: data[k])
    return list_ngrams
    '''
    all = []

    for pos_text in pos_texts:
        all.extend(get_skipgrams(pos_text, n, 3))

    data = dict(Counter(all))

    list_ngrams = heapq.nlargest(L, data.keys(), key=lambda k: data[k])
    return list_ngrams
    '''

def return_best_word_n_grams(n, L, tokens):

    all_ngrams = ngrams(tokens, n)

    data = dict(Counter(all_ngrams))
    list_ngrams = heapq.nlargest(L, data.keys(), key=lambda k: data[k])
    return list_ngrams

def return_best_n_grams(n, L, text):

    n_grams = ngrams(text, n)

    data = dict(Counter(n_grams))
    list_ngrams = heapq.nlargest(L, data.keys(), key=lambda k: data[k])
    return list_ngrams

def find_freq_n_gram_in_txt(text, pos_text, n_gram_lengths, n_grams, pos_n_gram_lengths, pos_n_grams, word_n_gram_lengths, word_n_grams):

    num_ngrams = sum([len(element) for element in n_grams]) + sum([len(element) for element in pos_n_grams]) + sum([len(element) for element in word_n_grams])
    to_ret = np.zeros([num_ngrams])
    ret_idx = 0

    #print("here")
    for idx in range(len(n_grams)):
        num_ngrams = len(Counter(ngrams(text, n_gram_lengths[idx])))

        for n_gram in n_grams[idx]:
            to_ret[ret_idx] = text.count(''.join(n_gram)) / num_ngrams if num_ngrams != 0 else 0
            ret_idx += 1

    #print("here")
    for idx in range(len(pos_n_grams)):
        num_pos_ngrams = len(Counter(get_skipgrams(pos_text, pos_n_gram_lengths[idx], 3)))

        for pos_n_gram in pos_n_grams[idx]:
            to_ret[ret_idx] = pos_text.count(''.join(pos_n_gram)) / num_pos_ngrams if num_pos_ngrams != 0 else 0
            ret_idx += 1

    #print("here")
    words = tokenize(text)
    spaced_text = ' '.join(words)
    for idx in range(len(word_n_grams)):
        num_word_ngrams = len(Counter(ngrams(words, word_n_gram_lengths[idx])))

        for word_ngram in word_n_grams[idx]:
            to_ret[ret_idx] = spaced_text.count(' '.join(word_ngram)) / num_word_ngrams if num_word_ngrams != 0 else 0
            ret_idx += 1

    return to_ret

def get_top_idx(freqs, k):

    freqs = [(idx, freqs[idx]) for idx in range(len(freqs))]
    freqs.sort(key = lambda val: val[1], reverse = True)

    return [freqs[idx][0] for idx in range(k)]

to_char = lambda x: tags[x] if x in tags else x
token_and_tag = lambda text: [tup[1] for tup in pos_tag(tokenize(text))]
token_tag_join = lambda text: ''.join([to_char(tag) for tag in token_and_tag(text)])
def tag_data(data):
    if 'POS_Tagged' not in data.columns:
        data['POS_text'] = data['text'].apply(token_tag_join)
    return data

def preprocess_data(data, num_authors, num_total, tag = True):

    print('------------', '\n', 'Tagging...')
    data = tag_data(data) if tag else data

    print('------------', '\n', 'Counting and aggregating texts...')
    number_texts = [0 for idx in range(num_total)]

    texts = ['' for idx in range(num_total)]
    pos_texts = ['' for idx in range(num_total)]

    for index, row in data.iterrows():
        author = int(row['label'])
        number_texts[author] += 1
        filtered_sentence = row['text'].replace('\n', '').strip()
        '''
        filtered_sentence = ' '.join([w for w in filtered_sentence.split() if not w in stop_words])
        filtered_sentence = ''.join([w for w in filtered_sentence if w not in set(punctuation)])
        '''
        texts[author] = ' '.join([texts[author], filtered_sentence])
        pos_texts[author] = ''.join([pos_texts[author], row['POS_text']])

    class_weights = [number_texts[author] for author in range(num_authors)]

    print('------------', '\n', 'Preprocessing complete!')

    return ' '.join(texts), ''.join(pos_texts), class_weights

def gen_data(data, lengths, n_grams, pos_lengths, pos_n_grams, word_n_gram_lengths, word_n_grams):

    print('------------', '\n', 'Generating data...')
    X = []
    y = []
    processed = 0
    for index, row in data.iterrows():
        if(processed % 1000 == 0):
            print(f"{processed} texts processed")

        y.append(int(row['label']))
        X.append(find_freq_n_gram_in_txt(row['text'], row['POS_text'], lengths, n_grams, pos_lengths, pos_n_grams, word_n_gram_lengths, word_n_grams))

        processed += 1

    X = np.array(X)
    y = np.array(y)

    return X, y #sklearn.model_selection.train_test_split(X, y, test_size = 0.15)

def gen_writeprints(data, top_idxs):
    print('------------', '\n', 'Generating WritePrints...')
    X = []
    y = []
    for index, row in data.iterrows():
        if(index % 1000 == 0):
            print(f"{index} texts processed")
        #if int(row[0]) in top_idxs:
        y.append(int(row['label']))#top_idxs[int(row['label'])])
        X.append(calculateFeatures(row['text']))
    return X, y

gc.collect()

train = pd.read_csv('/content/gdrive/MyDrive/PSU_REU/Data/TuringBench/ProcessedAA/train.csv').drop(['Unnamed: 0'], axis = 1)
test = pd.read_csv('/content/gdrive/MyDrive/PSU_REU/Data/TuringBench/ProcessedAA/test.csv').drop(['Unnamed: 0'], axis = 1)

train = train.rename(columns={'label': 'label', 'Generation': 'text', 'POS Tagged' : 'POS_text'})
test = test.rename(columns={'label': 'label', 'Generation': 'text', 'POS Tagged' : 'POS_text'})

train = pd.read_csv('/content/gdrive/MyDrive/PSU_REU/Data/Blog/train_obf.csv').drop(['Unnamed: 0'], axis = 1)
test = pd.read_csv('/content/gdrive/MyDrive/PSU_REU/Data/Blog/test.csv').drop(['Unnamed: 0'], axis = 1)

train = train.rename(columns={'label': 'label', 'Generation': 'text', 'POS Tagged' : 'POS_text'})
test = test.rename(columns={'label': 'label', 'Generation': 'text', 'POS Tagged' : 'POS_text'})
num_authors = 10

data = pd.read_csv('/content/gdrive/MyDrive/PSU_REU/Data/TuringBench/ChatGPT_AA.csv')
data = data.rename(columns={'Headline' : '_', 'Text': 'text', 'Label': 'label'})

train, test = sklearn.model_selection.train_test_split(data, test_size=0.2, random_state=1, shuffle=True, stratify=data['label'])
num_authors = 2

data.head()

num_authors = 2

'''
with open('/content/gdrive/MyDrive/PSU_REU/TextFooler/data/Blog_train.txt', 'w+') as writer:
    for _, row in train.iterrows():
        writer.write(''.join([str(row[0]), ' ', row[1], '\n']))
with open('/content/gdrive/MyDrive/PSU_REU/TextFooler/data/Blog_test.txt', 'w+') as writer:
    for _, row in test.iterrows():
        writer.write(''.join([str(row[0]), ' ', row[1], '\n']))
'''

def genPreds(data, tokenizer, classifier):

    preds = []

    for idx, row in data.iterrows():
        elem = row['Text']
        elem = tokenizer(elem, return_tensors="pt", padding="max_length", truncation=True, max_length=512).to(device)
        with torch.no_grad():
            logits = classifier(**elem).logits
        preds.append(logits.argmax().item())

        del elem
        del logits

        torch.cuda.empty_cache()

    torch.cuda.empty_cache()
    return preds

data = pd.read_csv('/content/gdrive/MyDrive/PSU_REU/Data/TuringBench/ChatGPT_AA.csv')

data.head()

preds = genPreds(data, tokenizer, classifier)

total = 0
correct = 0
for idx in range(0, len(preds)):
    if preds[idx] == data.iloc[idx, ]

total, pos_total, class_weights = preprocess_data(train, num_authors, num_authors, tag = True)
test = tag_data(test)

pos_total[:1000]

all_tokens = tokenize(total)

print('------------', '\n', 'Generating n-grams...')

n_grams = [return_best_n_grams(n, 250, total) for n in [1, 2, 3, 4]]

print('------------', '\n', 'Generating POS n-grams...')

pos_n_grams = [return_best_n_grams(n, 250, pos_total) for n in [1, 2, 3, 4]]

print('------------', '\n', 'Generating Word n-grams...')

word_n_grams = [return_best_word_n_grams(n, 250, all_tokens) for n in [1, 2, 3, 4]]

X_train, y_train = gen_data(train, [1, 2, 3, 4], n_grams, [1, 2, 3, 4], pos_n_grams, [1, 2, 3, 4], word_n_grams)
X_test, y_test = gen_data(test, [1, 2, 3, 4], n_grams, [1, 2, 3, 4], pos_n_grams, [1, 2, 3, 4], word_n_grams)

head = '/content/gdrive/MyDrive/PSU_REU/Models/TuringBench/POS_NN/BLIND'

if not os.path.isdir(head):
    os.makedirs(head)
pickle.dump(X_train, open(f"{head}X_train.pkl", "wb"))
pickle.dump(y_train, open(f"{head}y_train.pkl", "wb"))
pickle.dump(X_test, open(f"{head}X_test.pkl", "wb"))
pickle.dump(y_test, open(f"{head}y_test.pkl", "wb"))

features = [n_grams, pos_n_grams, word_n_grams]
pickle.dump(features, open(f"{head}features.pkl", "wb"))

X_train = pickle.load(open(f"{head}X_train.pkl", "rb"))
y_train = pickle.load(open(f"{head}y_train.pkl", "rb"))
X_test = pickle.load(open(f"{head}X_test.pkl", "rb"))
y_test = pickle.load(open(f"{head}y_test.pkl", "rb"))

Scaler = pickle.load(open(f"{head}Scaler.pkl", "rb"))
features = pickle.load(open(f"{head}features.pkl", "rb"))

'''
model = Model(len(X_train[0]), 2, Scaler)
model.load_state_dict(torch.load(f'{head}model.pt'))
model = model.cuda()
'''



EPOCHS = 50
STEP_SIZE = 3
GAMMA = 0.7
LR = 0.005
WEIGHT_DECAY = 0.000
MOMENTUM = 0.0
BATCH_SIZE = 2048

print('------------', '\n', 'Scaling, Loading, and Shuffling Data')
Scaler = sklearn.preprocessing.StandardScaler().fit(X_train)
X_train = Scaler.transform(X_train)
X_test = Scaler.transform(X_test)

training_Loader = Loader(X_train, y_train)
validation_Loader = Loader(X_test, y_test)

training_set = torch.utils.data.DataLoader(training_Loader, batch_size = BATCH_SIZE, shuffle = True)
validation_set = torch.utils.data.DataLoader(validation_Loader, batch_size = BATCH_SIZE, shuffle = False)

pickle.dump(Scaler, open(f"{head}Scaler.pkl", "wb"))
#pickle.dump(top_idxs, open(f"{head}top_idxs.pkl", "wb"))

print(len(X_train[0]))

model = Model(len(X_train[0]), num_authors)
#model.load_state_dict(torch.load('/content/gdrive/MyDrive/PSU_REU/Models/TuringBench/POS_NN/Blind_Black/model.pt'))

loss_function = nn.CrossEntropyLoss() #weight = torch.Tensor().to(device)
optimizer = optim.Adam(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)

model = train_and_eval(model, EPOCHS, training_set, validation_set, loss_function, optimizer, scheduler, head, 50)

from sklearn.svm import SVC
clf = SVC(random_state=0, verbose=True)
clf.fit(X_train, y_train)

preds = clf.predict(X_test)
correct = 0
total = 0

for idx in range(len(X_test)):
    if y_test[idx] == preds[idx]:
        correct += 1
    total += 1

print(correct / total)

from sklearn.svm import SVC
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.decomposition import PCA

X = X_test
y = y_test

pca = PCA(n_components=2)
Xreduced = pca.fit_transform(X)

model = SVC(random_state=0, verbose=True)
clf = model.fit(Xreduced, y)

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 300

fig, ax = plt.subplots()
# title for the plots
title = ('Decision surface of linear SVC ')
# Set-up grid for plotting.
X0, X1 = Xreduced[:, 0], Xreduced[:, 1]
xx, yy = make_meshgrid(X0, X1)

plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_ylabel('PC2')
ax.set_xlabel('PC1')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title('Decision Surface Visualization for SVM Model')
ax.legend()
plt.show()



#os.makedirs('/content/gdrive/MyDrive/PSU_REU/Models/BLOG/POS_NN/BLIND/Skip')
torch.save(model.state_dict(), f'{head}model.pt')

from captum.attr import IntegratedGradients

features = [n_grams, pos_n_grams, word_n_grams]

os.makedirs('/content/gdrive/MyDrive/PSU_REU/Data/BLOG/Blind/Skip')
np.save('/content/gdrive/MyDrive/PSU_REU/Data/BLOG/Blind/Skip/features.npy', features)

rev_tags = dict()
for tag in tags:
    rev_tags[tags[tag]] = tag

print(len(X_train[0]))

torch.cuda.empty_cache()

obs_set = torch.utils.data.DataLoader(validation_Loader, batch_size = 32, shuffle = False)

ig = IntegratedGradients(model)

import torch.distributed as dist
from torch import Tensor
from torch.multiprocessing import Process
import operator

import operator

obs_idx = 0

start = time.time()
all = []
torch.cuda.empty_cache()
for data, label in obs_set:

    attributions = ig.attribute(data.cuda(), target = label.to(torch.int64).cuda(), n_steps = 8192)
    attributions = attributions.tolist()

    for attribution in attributions:
        all.append(attribution)

    torch.cuda.empty_cache()
    del attributions

print(time.time() - start)

test['attributions'] = all

head

test.to_csv('/content/gdrive/MyDrive/PSU_REU/Data/TuringBench/test_attr_8192.csv')

print(test.head())

test.to_csv('/content/gdrive/MyDrive/PSU_REU/Data/TuringBench/Blind_Black/obsfucation_highstep.csv')

np.save('/content/gdrive/MyDrive/PSU_REU/Data/TuringBench/Blind_Black/features.npy', features)

print(split)

{k: v for k, v in sorted(x.items(), key=lambda item: item[1])}

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
import numpy as np
import nltk
import spacy
import re
from sortedcontainers import SortedDict
import os
from keras.preprocessing import text

nlp = spacy.load('en_core_web_sm')
cur_dir = "/content/gdrive/MyDrive/Colab Notebooks"

############## FEATURES COMPUTATION #####################
def getCleanText(inputText):
    # cleanText = text.text_to_word_sequence(inputText,filters='!#$%&()*+,-./:;<=>?@[\\]^_{|}~\t\n"\' ', lower=False, split=" ")
    cleanText = text.text_to_word_sequence(inputText, filters='', lower=False, split=" ")

    cleanText = ''.join(str(e) + " " for e in cleanText)
    return cleanText


def charactersCount(inputText):
    '''
    Calculates character count including spaces
    '''
    inputText = inputText.lower()
    charCount = len(str(inputText))
    return charCount


def averageCharacterPerWord(inputText):
    '''
    Calculates average number of characters per word
    '''

    words = text.text_to_word_sequence(inputText, filters=' !#$%&()*+,-./:;<=>?@[\\]^_{|}~\t\n"', lower=True, split=" ")
    inputText = inputText.lower().replace(" ", "")
    charCount = len(str(inputText))

    avgCharCount = charCount / len(words) if len(words) != 0 else 0
    return avgCharCount


def frequencyOfLetters(inputText):
    '''
    Calculates the frequency of letters
    '''

    inputText = str(inputText).lower()  # because its case sensitive
    inputText = inputText.lower().replace(" ", "")
    characters = "abcdefghijklmnopqrstuvwxyz"
    charsFrequencyDict = {}
    for c in range(0, len(characters)):
        char = characters[c]
        charsFrequencyDict[char] = 0
        for i in str(inputText):
            if char == i:
                charsFrequencyDict[char] = charsFrequencyDict[char] + 1

    # making a vector out of the frequencies
    vectorOfFrequencies = [0] * len(characters)
    totalCount = sum(list(charsFrequencyDict.values()))
    for c in range(0, len(characters)):
        char = characters[c]
        vectorOfFrequencies[c] = charsFrequencyDict[char] / totalCount if totalCount != 0 else 0

    return vectorOfFrequencies


def mostCommonLetterBigrams(inputText):
    # to do


    bigrams = ['th','he','in','er','an','re','nd','at','on','nt','ha','es','st' ,'en','ed','to','it','ou','ea','hi','is','or','ti','as','te','et' ,'ng','of','al','de','se','le','sa','si','ar','ve','ra','ld','ur']

    bigramsD = {}
    for t in bigrams:
        bigramsD[t] = True

    bigramCounts = {}
    for t in bigramsD:
        bigramCounts[t] = 0

    totalCount = 0
    words = text.text_to_word_sequence(inputText, filters=",.?!\"'`;:-()&$", lower=True, split=" ")
    for word in words:
        for i in range(0, len(word) - 1):
            bigram = str(word[i:i + 2]).lower()
            if bigram in bigrams:
                bigramCounts[bigram] = bigramCounts[bigram] + 1
                totalCount = totalCount + 1

    bigramsFrequency = []
    for t in bigrams:
        freq = float(bigramCounts[t] / totalCount) if totalCount != 0 else 0
        bigramsFrequency.append(freq)

    return bigramsFrequency


def mostCommonLetterTrigrams(inputText):
    # to do
    trigrams = ["the", "and", "ing", "her", "hat", "his", "tha", "ere", "for", "ent", "ion", "ter", "was", "you", "ith",
                "ver", "all", "wit", "thi", "tio"]
    trigramsD = {}
    for t in trigrams:
        trigramsD[t] = True

    trigramCounts = {}
    for t in trigramsD:
        trigramCounts[t] = 0

    totalCount = 0
    words = text.text_to_word_sequence(inputText, filters=",.?!\"'`;:-()&$", lower=True, split=" ")
    for word in words:
        for i in range(0, len(word) - 2):
            trigram = str(word[i:i + 3]).lower()
            if trigram in trigrams:
                trigramCounts[trigram] = trigramCounts[trigram] + 1
                totalCount = totalCount + 1

    trigramsFrequency = []
    for t in trigrams:
        freq = float(trigramCounts[t] / totalCount) if totalCount != 0 else 0
        trigramsFrequency.append(freq)

    return trigramsFrequency


def digitsPercentage(inputText):
    '''
    Calculates the percentage of digits out of total characters
    '''
    inputText = inputText.lower()
    charsCount = len(str(inputText))
    digitsCount = list([1 for i in str(inputText) if i.isnumeric() == True]).count(1)
    return digitsCount / charsCount if charsCount != 0 else 0


def charactersPercentage(inputText):
    '''
    Calculates the percentage of characters out of total characters
    '''

    inputText = inputText.lower().replace(" ", "")
    characters = "abcdefghijklmnopqrstuvwxyz"
    allCharsCount = len(str(inputText))
    charsCount = list([1 for i in str(inputText) if i in characters]).count(1)
    return charsCount / allCharsCount if allCharsCount != 0 else 0


def upperCaseCharactersPercentage(inputText):
    '''
    Calculates the percentage of uppercase characters out of total characters
    '''

    inputText = inputText.replace(" ", "")
    characters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    allCharsCount = len(str(inputText))
    charsCount = list([1 for i in str(inputText) if i in characters]).count(1)
    return charsCount / allCharsCount if allCharsCount != 0 else 0


def frequencyOfDigits(inputText):
    '''
    Calculates the frequency of digits
    '''

    digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    digitsCounts = {}
    for digit in digits:
        digitsCounts[str(digit)] = 0

    alldigits = re.findall('\d', inputText)
    for digit in alldigits:
        digitsCounts[digit] += 1

    digitsCounts = SortedDict(digitsCounts)
    digitsCounts = np.array(digitsCounts.values())
    return np.divide(digitsCounts, charactersCount(inputText))

def frequencyOfDigitsNumbers(inputText, digitLength):
    '''
    Calculates the frequency of digits
    '''

    inputText = str(inputText).lower()  # because its case sensitive
    words = text.text_to_word_sequence(inputText, filters=' !#$%&()*+,-./:;<=>?@[\\]^_{|}~\t\n"', lower=True, split=" ")

    count = 0
    wordCount = len(words)
    for w in words:
        if w.isnumeric() == True and len(w) == digitLength:
            count = count + 1

    return count / wordCount if wordCount != 0 else 0


def frequencyOfWordLength(inputText):
    '''
    Calculate frequency of words of specific lengths upto 15
    '''
    lengths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
    wordLengthFrequencies = {}
    for l in lengths:
        wordLengthFrequencies[l] = 0

    words = text.text_to_word_sequence(inputText, filters=' !#$%&()*+,-./:;<=>?@[\\]^_{|}~\t\n"', lower=True, split=" ")
    for w in words:
        wordLength = len(w)
        if wordLength in wordLengthFrequencies:
            wordLengthFrequencies[wordLength] = wordLengthFrequencies[wordLength] + 1

    frequencyVector = [0] * (len(lengths))
    totalCount = sum(list(wordLengthFrequencies.values()))
    for w in wordLengthFrequencies:
        frequencyVector[w - 1] = wordLengthFrequencies[w] / totalCount if totalCount != 0 else 0

    return frequencyVector


def frequencyOfSpecialCharacters(inputText):


    inputText = str(inputText).lower()  # because its case insensitive
    # inputText = inputText.lower().replace(" ", "")
    specialCharacters = open(cur_dir + "/writeprintresources/writeprints_special_chars.txt", "r").readlines()
    specialCharacters = [s.strip("\n") for s in specialCharacters]
    specialCharactersFrequencyDict = {}
    for c in range(0, len(specialCharacters)):
        specialChar = specialCharacters[c]
        specialCharactersFrequencyDict[specialChar] = 0
        for i in str(inputText):
            if specialChar == i:
                specialCharactersFrequencyDict[specialChar] = specialCharactersFrequencyDict[specialChar] + 1


    # making a vector out of the frequencies
    vectorOfFrequencies = [0] * len(specialCharacters)
    totalCount = sum(list(specialCharactersFrequencyDict.values())) + 1
    for c in range(0, len(specialCharacters)):
        specialChar = specialCharacters[c]
        vectorOfFrequencies[c] = specialCharactersFrequencyDict[specialChar] / totalCount if totalCount != 0 else 0

    vectorOfFrequencies = np.array(vectorOfFrequencies)
    return vectorOfFrequencies


def functionWordsPercentage(inputText):
    functionWords = open(cur_dir + "/writeprintresources/functionWord.txt", "r").readlines()
    functionWords = [f.strip("\n") for f in functionWords]
    # print((functionWords))
    words = text.text_to_word_sequence(inputText, filters=' !#$%&()*+,-./:;<=>?@[\\]^_{|}~\t\n"', lower=True, split=" ")
    frequencyOfFunctionWords = []
    for i in range(len(functionWords)):
        functionWord = functionWords[i]
        freq = 0
        for word in words:
            if word == functionWord:
                freq+=1
        frequencyOfFunctionWords.append(freq)
    # functionWordsIntersection = set(words).intersection(set(functionWords))

    return frequencyOfFunctionWords


def frequencyOfPunctuationCharacters(inputText):
    '''
    Calculates the frequency of special characters
    '''

    inputText = str(inputText).lower()  # because its case insensitive
    inputText = inputText.lower().replace(" ", "")
    specialCharacters = open(cur_dir + "/writeprintresources/writeprints_punctuation.txt", "r").readlines()
    specialCharacters = [s.strip("\n") for s in specialCharacters]
    specialCharactersFrequencyDict = {}
    for c in range(0, len(specialCharacters)):
        specialChar = specialCharacters[c]
        specialCharactersFrequencyDict[specialChar] = 0
        for i in str(inputText):
            if specialChar == i:
                specialCharactersFrequencyDict[specialChar] = specialCharactersFrequencyDict[specialChar] + 1

    # making a vector out of the frequencies
    vectorOfFrequencies = [0] * len(specialCharacters)
    totalCount = sum(list(specialCharactersFrequencyDict.values())) + 1
    for c in range(0, len(specialCharacters)):
        specialChar = specialCharacters[c]
        vectorOfFrequencies[c] = specialCharactersFrequencyDict[specialChar] / totalCount if totalCount != 0 else 0

    return vectorOfFrequencies


def misSpellingsPercentage(inputText):
    misspelledWords = open(cur_dir + "/writeprintresources/writeprints_misspellings.txt", "r").readlines()
    misspelledWords = [f.strip("\n") for f in misspelledWords]
    words = text.text_to_word_sequence(inputText, filters=' !#$%&()*+,-./:;<=>?@[\\]^_{|}~\t\n"', lower=True, split=" ")
    misspelledWordsIntersection = set(words).intersection(set(misspelledWords))
    return len(misspelledWordsIntersection) / len(list(words))


def legomena(inputText):
    freq = nltk.FreqDist(word for word in inputText.split())
    hapax = [key for key, val in freq.items() if val == 1]
    dis = [key for key, val in freq.items() if val == 2]
    try:
        return list((len(hapax) / len(inputText.split()),len(dis)/ len(inputText.split())))
    except:
        return [0,0]


def posTagFrequency(inputText):
    pos_tags = []
    doc = nlp(str(inputText))
    for token in doc:
        pos_tags.append(str(token.pos_))

    # tagset = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']
    tagset = ['ADJ', 'ADP', 'ADV','AUX', 'CONJ','CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'SPACE', 'X']
    tags = [tag for tag in pos_tags]
    return list(tuple(tags.count(tag) / len(tags) for tag in tagset))

def totalWords(inputText):
    words = text.text_to_word_sequence(inputText, filters=",.?!\"'`;:-()&$", lower=True, split=" ")
    return len(words)

def averageWordLength(inputText):
    words = text.text_to_word_sequence(inputText, filters=",.?!\"'`;:-()&$", lower=True, split=" ")
    lengths = []
    for word in words:
        lengths.append(len(word))
    return np.mean(lengths)

def noOfShortWords(inputText):
    words = text.text_to_word_sequence(inputText, filters=",.?!\"'`;:-()&$", lower=True, split=" ")
    shortWords = []
    for word in words:
        if len(word) <= 3:
            shortWords.append(word)
    return len(shortWords)

def calculateFeatures(inputText):


    featureList = []

    ## GROUP 1
    featureList.extend([totalWords(inputText)])
    featureList.extend([averageWordLength(inputText)])
    featureList.extend([noOfShortWords(inputText)])
    ## GROUP 2
    featureList.extend([charactersCount(inputText)])
    featureList.extend([digitsPercentage(inputText)])
    featureList.extend([upperCaseCharactersPercentage(inputText)])
    ## GROUP 3
    featureList.extend(frequencyOfSpecialCharacters(inputText))
    ## GROUP 4
    featureList.extend(frequencyOfLetters(inputText))
    ## GROUP 5
    featureList.extend(frequencyOfDigits(inputText))
    ## GROUP 6
    featureList.extend(mostCommonLetterBigrams(inputText))
    ## GROUP 7
    featureList.extend(mostCommonLetterTrigrams(inputText))
    ## GROUP 8
    featureList.extend(legomena(inputText))
    ## GROUP 9
    featureList.extend(functionWordsPercentage(inputText))
    ## GROUP 10
    featureList.extend(posTagFrequency(inputText))
    ## GROUP 11
    featureList.extend(frequencyOfPunctuationCharacters(inputText))

    return featureList
######################################## TRAINING CLASSIFIER ###########################